
# 📌 서포트 벡터 머신 (SVM)


<details>
<summary>내용보기 🔽</summary>
  
- 참고 : [내용](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-2%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0-SVM), [영상자료](https://www.youtube.com/watch?v=y4CYcpRiNsw)
- 분류(Classification)와 회귀(Regression)에 사용되는 **지도 학습** 모델
- SVM의 핵심 목표는 주어진 데이터를 가장 효과적으로 나눌 수 있는 **최적의 결정 경계**를 찾는 것
> <img width="400" height="400" alt="image" src="https://github.com/user-attachments/assets/9e751397-691f-4e0b-8f6b-8e64a74deae0" />

## ✔ 작동원리
- SVM은 데이터를 두 클래스로 나누는 '결정 경계(Decision Boundary)'를 찾는다.
- 단순히 데이터를 나누는 선만 찾는 것이 아니라, **가장 넓은 마진(Margin)**을 확보하는 결정 경계를 찾는다.

## ✔ K-NN과의 차이점
- **SVM**: 학습이 느리지만 예측이 빠르고, 메모리 효율적. 고차원 데이터에 강함.
- **KNN**: 학습은 빠르지만 예측이 느리고, 데이터 양 많으면 성능 저하. 간단하고 직관적.


| **특징**          | **SVM (Support Vector Machine)** | **KNN (K-Nearest Neighbors)** |
| --------------- | -------------------------------- | ----------------------------- |
| **알고리즘 타입**     | 지도 학습, 분류 및 회귀 가능                | 지도 학습, 분류 및 회귀 가능             |
| **학습 방식**       | **모델 기반 학습**: 초평면(Hyperplane) 생성 | **비모델 기반 학습**: 학습 단계에서 모델 없음  |
| **학습 속도**       | 느림 (복잡한 최적화 과정 필요)               | 매우 빠름 (거의 없음)                 |
| **예측 속도**       | 빠름 (수학적 모델로 바로 예측 가능)            | 느림 (예측 시 모든 데이터와 거리 계산 필요)    |
| **메모리 사용량**     | 적음 (지원 벡터만 저장)                   | 많음 (전체 학습 데이터를 저장)            |
| **핵심 아이디어**     | 데이터를 구분하는 최적의 결정 경계(초평면) 찾기      | 입력 데이터와 가장 가까운 K개의 이웃을 찾기     |
| **거리 개념 사용 여부** | 선택적 (커널에 따라 다름)                  | 필수 (유클리드 거리, 맨해튼 거리 등)        |
| **커널 지원**       | 지원 (선형, RBF, 다항식 등 다양한 커널 가능)    | 없음                            |
| **하이퍼파라미터**     | C, 커널 종류, 감마 등                   | K값, 거리 측정 방식                  |
| **고차원 데이터**     | 매우 잘 작동 (차원의 저주에 덜 민감)           | 차원의 저주에 취약                    |
| **적합한 문제 유형**   | 고차원, 명확한 경계가 있는 데이터              | 소규모 데이터, 로컬 패턴이 중요한 데이터       |


</details>
  
---

# 📌 HOG(Histogram of Oriented Gradient) 디스크립터

<details>
<summary>내용보기 🔽</summary>
  
- 참고 : [내용](https://bkshin.tistory.com/entry/OpenCV-33-HOG-%EB%94%94%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%84%B0HOG-Descriptor)
- 이미지의 국소 영역에서 방향성(Gradient) 정보를 히스토그램 형태로 표현하는 특징 추출 기법
- 보행자 검출을 위해 만들어진 특징 디스크립터
> <img width="500" height="589" alt="image" src="https://github.com/user-attachments/assets/d36301a9-0570-4e6d-a3c4-7527a65e1246" />


## ✔ 특징
- 로컬 패턴 보존: 국소 영역의 모양 정보를 효과적으로 캡처.
- 밝기 변화에 강함: 픽셀 값 대신 기울기 사용.
- 회전 변화에 약함: 객체가 회전하면 성능 저하 가능.
- 계산량 많음: 이미지 전체의 모든 블록에 대해 계산 필요.

## ✔ 장단점
| 장점                | 단점                  |
| ----------------- | ------------------- |
| 조명 변화에 강함         | 계산량이 많음             |
| 단순한 구조            | 회전/크기 변화에 취약        |
| SVM 등과 결합 시 성능 우수 | 딥러닝 등장 이후 상대적 비중 감소 |


</details>

---

# 📌 BOW( Bag of Words)

<details>
<summary>내용보기 🔽</summary>
  
- 참고 : [내용](http://atonrq.synology.me:1700/hypha/bow)
- 우리 말로 하면 단어 주머니. 원래 문서 분류에 사용하던 알고리즘
- 문장이나 문서의 단어 순서는 무시하고 단어들의 출현 빈도에만 집중하여 텍스트를 수치 데이터로 표현하는 모델


## ✔ BOW 알고리즘과 객체 인식 
- 낱말 대신 `SIFT`, `SURF`와 같은 특징 스크립터를 사용
- 예: 영상에 있는 비행기와 모터사이클을 분류할 때
  1. SIFT와 같은 특징 스크립터를 계산해서 하나의 저장 공간에 차곡차곡 모은다.
  2. 마구잡이로 뒤섞여 있는 수많은 비행기와 모터사이클의 특징을 디스크립터를 k-means 클러스터 알고리즘으로 군집화.
  3. 각각의 비행기와 모터사이클 사진에 대해서 시각 사전을 만들 떄와 같은 특징 디스크립터를 계산해서 시각 사전에 등록된 특징 디스크립터와 매칭되는 것이 얼마나 되는지 히스토램을 작성하고 레이블을 만들어 짝지어 준비.

> <img width="600" height="796" alt="image" src="https://github.com/user-attachments/assets/f2d87fab-210e-4c7b-8dcb-0c61f2965be6" />

## ✔ 특징
- 단어 순서 무시: BOW 모델은 문장의 문법이나 단어의 순서(예: "나는 사과를 먹는다"와 "사과를 나는 먹는다")를 무시한다. 단지 어떤 단어가 얼마나 많이 나타났는지에만 집중한다.
- 단어 벡터 : 각 문서를 고정된 길이의 벡터로 표현
- 빈도수 : 각 문서 벡터의 값은 해당 문서에 특정 단어가 나타난 횟수(빈도수)를 의미한다.

BOW는 단순한 모델이지만, 문서 분류(Document Classification)와 같은 작업에서 꽤 좋은 성능을 보여주며, 다른 복잡한 NLP 모델의 기초가 된다.

</details>

---




